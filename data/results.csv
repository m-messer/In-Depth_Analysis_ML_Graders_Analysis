authors,doi,journal,abstract,city,pages,issn,url,id,publisher,volume,note,type,keywords,year,issue,title,month,isbn,source,decision,approach,category,model,preprocessing,skill,evaluation,eval_dataset
"Ramez Nabil, Nour Eldeen Mohamed, Ahmed Mahdy, Khaled Nader, Shereen Essam, Essam Eliwa",10.1109/MIUCC52538.2021.9447629,"2021 International Mobile, Intelligent, and Ubiquitous Computing Conference, MIUCC 2021","Continuous evaluation of computer programs and providing informative assessments are crucial for computer programming students. However, swift and formative feedback can be challenging to achieve as it is usually a stressful and tedious task for professors merely through manual grading. There is an urgent need for a Learning management system (LMS) that offers instant and detailed feedback in a competitive environment for a better education experience. In this study, we introduce the EvalSeer learning management system. EvalSeer is an LMS equipped with an intelligent auto-grading engine to keep learners motivated and help them move forward. The code evaluation process covers various criteria that strengthen coding abilities and provides learners with the directions they need to improve. These criteria include coding style, code features, dynamic test cases, and successful compilation. EvalSeer uses Long short-term memory (LSTM) networks for code analysis to detect syntax errors and predict potential fixes. Also, the system shall explain suggested fixes backed up with related references. EvalSeer is an easy-to-use cloud-based system with a learner-first approach that can be applied both on-campus and in elearning systems. This work is timely with the dramatic education change, with a notable rise of e-learning due to the COVID-19 pandemic.",,235-242,,,rayyan-354359330,Institute of Electrical and Electronics Engineers Inc.,,,article,"Automated assessment,Deep learning,competitive learning,formative feedback,gamification,programming education",2021,,EvalSeer: An Intelligent Gamified System for Programming Assignments Assessment,5,9781665412438,SLR,Include,Supervised,Neural,LSTM,Remove comments; Normalize tokens,Correctness,Accuracy,Github
"Rahul Gupta, Soham Pal, Aditya Kanade, Shirish Shevade",10.1609/AAAI.V31I1.10742,Proceedings of the AAAI Conference on Artificial Intelligence,"The problem of automatically fixing programming errors is a very active research topic in software engineering. This is a challenging problem as fixing even a single error may require analysis of the entire program. In practice, a number of errors arise due to programmer's inexperience with the programming language or lack of attention to detail. We call these common programming errors. These are analogous to grammatical errors in natural languages. Compilers detect such errors, but their error messages are usually inaccurate. In this work, we present an end-to-end solution, called DeepFix, that can fix multiple such errors in a program without relying on any external tool to locate or fix them. At the heart of DeepFix is a multi-layered sequence-to-sequence neural network with attention which is trained to predict erroneous program locations along with the required correct statements. On a set of 6971 erroneous C programs written by students for 93 programming tasks, DeepFix could fix 1881 (27%) programs completely and 1338 (19%) programs partially.",,1345-1351,2374-3468,https://ojs.aaai.org/index.php/AAAI/article/view/10742,Gupta2017,AAAI press,31,,article,"deep learning,fault localization,program repair,programming education",2017,1,DeepFix: Fixing Common C Language Errors by Deep Learning,2,,Backward,Include,Supervised,Neural,Adapted seq2seq attention model,Normalize tokens; Encode Line Numbers,Syntactic Correctness,Accuracy,Internal
"Rahul Gupta, Aditya Kanade, Shirish Shevade",10.48550/arxiv.1801.10467,,"Novice programmers often struggle with the formal syntax of programming languages. To assist them, we design a novel programming language correction framework amenable to reinforcement learning. The framework allows an agent to mimic human actions for text navigation and editing. We demonstrate that the agent can be trained through self-exploration directly from the raw input, that is, program text itself, without any knowledge of the formal syntax of the programming language. We leverage expert demonstrations for one tenth of the training data to accelerate training. The proposed technique is evaluated on 6975 erroneous C programs with typographic errors, written by students during an introductory programming course. Our technique fixes 14% more programs and 29% more compiler error messages relative to those fixed by a state-of-the-art tool, DeepFix, which uses a fully supervised neural machine translation approach.",,,,https://arxiv.org/abs/1801.10467v1,Gupta2018,,,,article,,2018,,Deep Reinforcement Learning for Programming Language Correction,1,,Backward,Exclude - Duplicate,Semi-Supervised,Neural and Agent-Based,LSTM for embedding and A3C for localisation and repair,Normalize names and literals,Syntactic Correctness,"Accuracy, compared to other baseline models",Same as DeepFix
"Sahil Bhatia, Rishabh Singh",,,"We present a method for automatically generating repair feedback for syntax errors for introductory programming problems. Syntax errors constitute one of the largest classes of errors (34%) in our dataset of student submissions obtained from a MOOC course on edX. The previous techniques for generating automated feed- back on programming assignments have focused on functional correctness and style considerations of student programs. These techniques analyze the program AST of the program and then perform some dynamic and symbolic analyses to compute repair feedback. Unfortunately, it is not possible to generate ASTs for student pro- grams with syntax errors and therefore the previous feedback techniques are not applicable in repairing syntax errors. We present a technique for providing feedback on syntax errors that uses Recurrent neural networks (RNNs) to model syntactically valid token sequences. Our approach is inspired from the recent work on learning language models from Big Code (large code corpus). For a given programming assignment, we first learn an RNN to model all valid token sequences using the set of syntactically correct student submissions. Then, for a student submission with syntax errors, we query the learnt RNN model with the prefix to- ken sequence to predict token sequences that can fix the error by either replacing or inserting the predicted token sequence at the error location. We evaluate our technique on over 14, 000 student submissions with syntax errors. Our technique can completely re- pair 31.69% (4501/14203) of submissions with syntax errors and in addition partially correct 6.39% (908/14203) of the submissions.",,,,https://arxiv.org/abs/1603.06129v1,Bhatia2016,,,,article,,2016,,Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks,3,,Backward,Exclude - Duplicate,Supervised,Neural,RNN,Normalize rarely used tokens,Syntactic Correctness,"Accuracy, compared to other baseline models",Unknown
"Rahul Gupta, Aditya Kanade, Shirish Shevade",10.1609/AAAI.V33I01.3301930,Proceedings of the AAAI Conference on Artificial Intelligence,"Novice programmers often struggle with the formal syntax of programming languages. In the traditional classroom setting, they can make progress with the help of real time feedback from their instructors which is often impossible to get in the massive open online course (MOOC) setting. Syntactic error repair techniques have huge potential to assist them at scale. Towards this, we design a novel programming language correction framework amenable to reinforcement learning. The framework allows an agent to mimic human actions for text navigation and editing. We demonstrate that the agent can be trained through self-exploration directly from the raw input, that is, program text itself, without either supervision or any prior knowledge of the formal syntax of the programming language. We evaluate our technique on a publicly available dataset containing 6975 erroneous C programs with typographic errors, written by students during an introductory programming course. Our technique fixes 1699 (24.4%) programs completely and 1310 (18.8%) program partially, outperforming DeepFix, a state-of-the-art syntactic error repair technique, which uses a fully supervised neural machine translation approach.",,930-937,2374-3468,https://ojs.aaai.org/index.php/AAAI/article/view/3882,Gupta2019a,AAAI Press,33,,article,,2019,1,Deep Reinforcement Learning for Syntactic Error Repair in Student Programs,7,9781577358091,Backward,Include,Semi-Supervised; Reinforcement Learning,Neural,LSTM for embedding and A3C for localisation and repair,Normalize tokens,Syntactic Correctness,Accuracy; Compared to baselines,Same as DeepFix
"Sahil Bhatia, Pushmeet Kohli, Rishabh Singh",10.1145/3180155.3180219,Proceedings - International Conference on Software Engineering,"Automatic correction of programs is a challenging problem with numerous real world applications in security, verification, and education. One application that is becoming increasingly important is the correction of student submissions in online courses for providing feedback. Most existing program repair techniques analyze Abstract Syntax Trees (ASTs) of programs, which are unfortunately unavailable for programs with syntax errors. In this paper, we propose a novel Neuro-symbolic approach that combines neural networks with constraint-based reasoning. Specifically, our method first uses a Recurrent Neural Network (RNN) to perform syntax repairs for the buggy programs; subsequently, the resulting syntactically-fixed programs are repaired using constraint-based techniques to ensure functional correctness. The RNNs are trained using a corpus of syntactically correct submissions for a given programming assignment, and are then queried to fix syntax errors in an incorrect programming submission by replacing or inserting the predicted tokens at the error location. We evaluate our technique on a dataset comprising of over 14,500 student submissions with syntax errors. Our method is able to repair syntax errors in 60% (8689) of submissions, and finds functionally correct repairs for 23.8% (3455) submissions.",,60-70,2705257,,Bhatia2018,IEEE Computer Society,2018-January,,article,"Automated Feedback Generation,Neural Program Correction,Neural guided search",2018,,Neuro-Symbolic Program Corrector for Introductory Programming Assignments,,,Backward,Include,Supervised,Neural,RNN,Normalize tokens,Syntactic Correctness,Accuracy; Compared to baselines,Unknown
"Timotej Lazar, Martin Možina, Ivan Bratko",10.1007/978-3-319-61425-0_14/TABLES/1,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"When implementing a programming tutor it is often difficult to manually consider all possible errors encountered by students. An alternative is to automatically learn a bug library of erroneous patterns from students’ programs. We propose abstract-syntax-tree (AST) patterns as features for learning rules to distinguish between correct and incorrect programs. We use these rules to debug student programs: rules for incorrect programs (buggy rules) indicate mistakes, whereas rules for correct programs group programs with the same solution strategy. To generate hints, we first check buggy rules and point out incorrect patterns. If no buggy rule matches, we use rules for correct programs to recognize the student’s intent and suggest missing patterns. We evaluate our approach on past student programming data for a number of Prolog problems. For 31 out of 44 problems, the induced rules correctly classify over 85% of programs based only on their structural features. For approximately 73% of incorrect submissions, we are able to generate hints that were implemented by the student in some subsequent submission.",,162-174,16113349,https://link.springer.com/chapter/10.1007/978-3-319-61425-0_14,rayyan-354359414,Springer Verlag,10331 LNAI,,article,"Abstract syntax tree,Error diagnosis,Hint generation,Programming tutors,Syntactic features",2017,,Automatic extraction of AST patterns for debugging student programs,,9783319614243,SLR,Include,Supervised,Traditional,"Random Forest, Majority Classifer","Convert to Source to Graph (AST, CFG or DDG); Build good set using unit tests",Correctness,Accuracy; Compared to models within paper,Internal
"Umair Z. Ahmed, Renuka Sindhgatta, Nisheeth Srivastava, Amey Karkare",10.1109/ASE.2019.00039,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019","We present TEGCER, an automated feedback tool for novice programmers. TEGCER uses supervised classification to match compilation errors in new code submissions with relevant pre-existing errors, submitted by other students before. The dense neural network used to perform this classification task is trained on 15000+ error-repair code examples. The proposed model yields a test set classification Pred@3 accuracy of 97.7% across 212 error category labels. Using this model as its base, TEGCER presents students with the closest relevant examples of solutions for their specific error on demand. A large scale (N>230) usability study shows that students who use TEGCER are able to resolve errors more than 25% faster on average than students being assisted by human tutors.",,327-338,,,rayyan-354359367,Institute of Electrical and Electronics Engineers Inc.,,,article,"Compilation Error,Example Generation,Intelligent Tutoring Systems,Introductory Programming,Neural Networks",2019,,Targeted example generation for compilation errors,11,9781728125084,SLR,Include,Supervised,Neural,"Dense NN, 1 hidden layer",Normalize tokens,Syntactic Correctness,Accuracy; Precision; Recall; Case Study,Internal
"Melissa Day, Manohara Rao Penumala, Javier Gonzalez-Sanchez",10.1109/COGMI48466.2019.00018,"Proceedings - 2019 IEEE 1st International Conference on Cognitive Machine Intelligence, CogMI 2019","With Computer Science (CS) class sizes that are often large, it is challenging to provide effective personalized feedback to students. Intelligent Tutoring Companions can provide such feedback and improve CS students' experience. This work describes the construction of a Tutoring Companion, Annete, designed to support students in a university Java programming course by providing them with intelligent feedback generated by a neural network. Annete is embedded into the Eclipse Integrated Development Environment (IDE), which is an environment that is already familiar to students in programming courses. Embedding Annete into Eclipse improves her effectiveness, as the students do not need to learn how to use an additional tool. While the student works in Eclipse, Annete collects 21 pieces of data from the student's code, including whether certain key words are used, error messages from the compiler, and cyclomatic complexity. When a run attempt, debug attempt, or a request for help occurs in Eclipse, Annete uses the data available to infer a feedback message to show to the student. Our approach is evaluated among 28 CS students completing a programming assignment while Annete assists them. Results suggest that students feel supported while working with Annete and show potential for using neural network modeling with embedded tutoring companions in the future. Challenges are discussed, as well as opportunities for future work.",,71-80,,,rayyan-354359369,Institute of Electrical and Electronics Engineers Inc.,,,article,"Eclipse IDE,Intelligent tutoring systems,Neural networks,Teaching programming,Tutoring companions",2019,,Annete: An intelligent tutoring companion embedded into the eclipse IDE,12,9781728167374,SLR,Include,Supervised,Neural,MLP,Normalize tokens,Correctness and Maintainability,Case Study,Random generated
"Rahul Gupta, Aditya Kanade, Shirish Shevade",,Advances in Neural Information Processing Systems,"Providing feedback is an integral part of teaching. Most open online courses on programming make use of automated grading systems to support programming assignments and give real-time feedback. These systems usually rely on test results to quantify the programs' functional correctness. They return failing tests to the students as feedback. However, students may find it difficult to debug their programs if they receive no hints about where the bug is and how to fix it. In this work, we present NeuralBugLocator, a deep learning based technique, that can localize the bugs in a faulty program with respect to a failing test, without even running the program. At the heart of our technique is a novel tree convolutional neural network which is trained to predict whether a program passes or fails a given test. To localize the bugs, we analyze the trained network using a state-of-the-art neural prediction attribution technique and see which lines of the programs make it predict the test outcomes. Our experiments show that NeuralBugLocator is generally more accurate than two state-of-the-art program-spectrum based and one syntactic difference based bug-localization baselines.",,,,https://bitbucket.,Gupta2019b,,32,,article,,2019,,Neural Attribution for Semantic Bug-Localization in Student Programs,,,SLR,Include,Supervised,Neural,Tree CNN and property attribution model,"Convert to Source to Graph (AST, CFG or DDG); Normalize tokens",Correctness,Compared to baselines; Accuracy,Internal
"Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, Leonidas Guibas",,,"Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University's CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions.",,1093-1102,1938-7228,https://proceedings.mlr.press/v37/piech15.html,Piech2015,PMLR,,,generic,,2015,,Learning Program Embeddings to Propagate Feedback on Student Code,6,,Backward,Include,Supervised,Neural,Encoder/Decoder for embedding and RNN for prediction and feedback,"Convert to Source to Graph (AST, CFG or DDG)",Correctness and readability,Compared to baselines; Accuracy; Precision; Recall; Compare models within paper; Compared to metrics,Hour of Code
"Mike Wu, Milan Mosse, Noah Goodman, Chris Piech",10.1609/AAAI.V33I01.3301782,Proceedings of the AAAI Conference on Artificial Intelligence,"In modern computer science education, massive open online courses (MOOCs) log thousands of hours of data about how students solve coding challenges. Being so rich in data, these platforms have garnered the interest of the machine learning community, with many new algorithms attempting to autonomously provide feedback to help future students learn. But what about those first hundred thousand students? In most educational contexts (i.e. classrooms), assignments do not have enough historical data for supervised learning. In this paper, we introduce a human-in-the-loop “rubric sampling” approach to tackle the “zero shot” feedback challenge. We are able to provide autonomous feedback for the first students working on an introductory programming assignment with accuracy that substantially outperforms data-hungry algorithms and approaches human level fidelity. Rubric sampling requires minimal teacher effort, can associate feedback with specific parts of a student’s solution and can articulate a student’s misconceptions in the language of the instructor. Deep learning inference enables rubric sampling to further improve as more assignment specific student data is acquired. We demonstrate our results on a novel dataset from Code.org, the world’s largest programming education platform.",,782-790,2374-3468,https://ojs.aaai.org/index.php/AAAI/article/view/3857,Wu2019,AAAI Press,33,,article,,2019,1,Zero Shot Learning for Code Education: Rubric Sampling with Deep Learning Inference,7,9781577358091,SLR,Include,Unsupervised,Neural,MVAE,Convert blocks to string; create rubric with context-free-grammar,Correctness,Compared to baselines; Compared to human; Compared to models within paper; F1 score; Accuracy,code.org
"Elena L. Glassman, Rishabh Singh, Robert C. Miller",10.1145/2556325.2567865,L@S 2014 - Proceedings of the 1st ACM Conference on Learning at Scale,"Open-ended homework problems such as coding assignments give students a broad range of freedom for the design of solutions. We aim to use the diversity in correct solutions to enhance student learning by automatically suggesting alternate solutions. Our approach is to perform a two-level hierarchical clustering of student solutions to first partition them based on the choice of algorithm and then partition solutions implementing the same algorithm based on low-level implementation details. Our initial investigations in domains of introductory programming and computer architecture demonstrate that we need two different classes of features to perform effective clustering at the two levels, namely abstract features and concrete features.",,171-172,,https://dl.acm.org/doi/10.1145/2556325.2567865,Glassman2014,Association for Computing Machinery,,,article,"Algorithm recognition,Feature engineering,Program comprehension",2014,,Feature engineering for clustering student solutions,,,Backward,Include,Unsupervised,Traditional,k-Means,"Convert to Source to Graph (AST, CFG or DDG)",Correctness,Compared to human; Adjusted Mutual Information,Internal
"Fábio Rezende De Souza, Francisco De Assis Zampirolli, Guiou Kobayashi",10.5220/0007711000620069,CSEDU 2019 - Proceedings of the 11th International Conference on Computer Supported Education,"Thousands of students have their assignments evaluated by their teachers every day around the world while developing their studies in any branch of science. A fair evaluation of their schoolwork is a very challenging task. Here we present a method for validating the grades attributed by professors to students programming exercises in an undergraduate introductory course in computer programming. We collected 938 final exam exercises in Java Language developed during this course, evaluated by different professors, and trained a convolutional neural network over those assignments. First, we submit their codes to a cleaning process (by removing comments and anonymizing variables). Next, we generated an embedding representation of each source code produced by students. Finally, this representation is taken as the input of the neural network which classifies each label (corresponding to the possible grades A, B, C, D or F). An independent neural network is trained with source code solutions corresponding to each assignment. We obtained an average accuracy of 74.9% in a 10−fold cross validation for each grade. We believe that this method can be used to validate the grading process made by professors in order to detect errors that might happen during this process.",,62-69,,,rayyan-354359295,SciTePress,1,,article,"Artificial Intelligence,Automatic Grading,Deep Learning,Text Classification",2019,,Convolutional neural network applied to code assignment grading,,9789897583674,SLR,Include,Supervised,Neural,CNN,Remove comments; Normalize tokens,Correctness,Cross Validation; Accuracy,Internal
"Gursimran Singh, Shashank Srikant, Varun Aggarwal",10.1145/2939672.2939696,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,"Learning supervised models to grade open-ended responses is an expensive process. A model has to be trained for every prompt/question separately, which in turn requires graded samples. In automatic programming evaluation specifically, the focus of this work, this issue is amplified. The models have to be trained not only for every question but also for every language the question is offered in. Moreover, the availability and time taken by experts to create a labeled set of programs for each question is a major bottleneck in scaling such a system. We address this issue by presenting a method to grade computer programs which requires no manually assigned labeled samples for grading responses to a new, unseen question. We extend our previous work [25] wherein we introduced a grammar of features to learn question specific models. In this work, we propose a method to transform those features into a set of features that maintain their structural relation with the labels across questions. Using these features we learn one supervised model, across questions for a given language, which can then be applied to an ungraded response to an unseen question. We show that our method rivals the performance of both, question specific models and the consensus among human experts while substantially outperforming extant ways of evaluating codes. We demonstrate the system's value by deploying it to grade programs in a high stakes assessment. The learning from this work is transferable to other grading tasks such as math question grading and also provides a new variation to the supervised learning approach.",,263-272,,,Singh2016,Association for Computing Machinery,13-17-August-2016,,article,"Automatic grading,Feature engineering,MOOC,One-class learning,Question independent learning,Recruitment,Supervised learning",2016,,Question independent grading using machine learning: The case of computer program grading,8,9781450342322,SLR,Include,Supervised,Traditional,LASSO,Convert to Grammar; Build good set using unit tests,Correctness,Cross Validation; Pearson Correlation Coefficient; Bias; Mean Absolute Error; Compared to models within paper,Automata
"Shashank Srikant, Varun Aggarwal",10.1145/2623330.2623377,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,"The automatic evaluation of computer programs is a nascent area of research with a potential for large-scale impact. Extant program assessment systems score mostly based on the number of test-cases passed, providing no insight into the competency of the programmer. In this paper, we present a system to grade computer programs automatically. In addition to grading a program on its programming practices and complexity, the key kernel of the system is a machine-learning based algorithm which determines closeness of the logic of the given program to a correct program. This algorithm uses a set of highly-informative features, derived from the abstract representations of a given program, that capture the program's functionality. These features are then used to learn a model to grade the programs, which are built against evaluations done by experts. We show that the regression models provide much better grading than the ubiquitous test-case-pass based grading and rivals the grading accuracy of other open-response problems such as essay grading . We also show that our novel features add significant value over and above basic keyword/expression count features. In addition to this, we propose a novel way of posing computer-program grading as a one-class modeling problem and report encouraging preliminary results. We show the value of the system through a case study in a real-world industrial deployment. To the best of the authors' knowledge, this is the first time a system using machine learning has been developed and used for grading programs. The work is timely with regard to the recent boom in Massively Online Open Courseware (MOOCs), which promises to produce a significant amount of hand-graded digitized data. © 2014 ACM.",,1887-1896,,https://dl.acm.org/doi/10.1145/2623330.2623377,Srikant2014,Association for Computing Machinery,,,article,"MOOC,One-class learning,Program Analysis,Supervised learning,automatic grading,feature engineering,mooc,one-class learning,recruitment,supervised learning",2014,,A system to grade computer programming skills using machine learning,,9781450329569,Backward,Include,Supervised,Traditional,Ridge Regression and others,"Count Tokens; Count Expressions; Build good set using unit tests; Convert to Source to Graph (AST, CFG or DDG)",Correctness,Cross Validation; Pearson Correlation Coefficient; Compared to models within paper; Case Study,Internal
"Arjun Verma, Prateksha Udhayanan, Rahul Murali Shankar, Nikhila Kn, Sujit Kumar Chakrabarti",10.1145/3486001.3486228,ACM International Conference Proceeding Series,"A majority of the current automated evaluation tools focus on grading a program based only on functionally testing the outputs. This approach suffers both false positives (i.e. finding errors where there are not any) and false negatives (missing out on actual errors). In this paper, we present a novel system which emulates manual evaluation of programming assignments based on the structure and not the functional output of the program using structural similarity between the given program and a reference solution. We propose an evaluation rubric for scoring structural similarity with respect to a reference solution. We present an ML based approach to map the system predicted scores to the scores computed using the rubric. Empirical evaluation of the system is done on a corpus of Python programs extracted from the popular programming platform, HackerRank, in combination with programming assignments submitted by students undertaking an undergraduate Python programming course. The preliminary results have been encouraging with the errors reported being as low as 12 percent with a deviation of about 3 percent, showing that the automatically generated scores are in high correlation with the instructor assigned scores.",,,,https://dl.acm.org/doi/10.1145/3486001.3486228,rayyan-354359341,Association for Computing Machinery,,,article,"Automated Evaluation,Evaluation Rubric,Program Structural Similarity,Syntax Tree Fingerprinting",2021,,Source-Code Similarity Measurement: Syntax Tree Fingerprinting for Automated Evaluation,10,9781450385947,SLR,Include,Supervised,Traditional,SVM,"Convert to Source to Graph (AST, CFG or DDG); Normalize tokens",Correctness,Mean Absolute Error; Root Mean Square Error; Cross Validation; Compared to human,Hackerrank and internal
"Eddie Antonio Santos, Joshua Charles Campbell, Dhvani Patel, Abram Hindle, Jose Nelson Amaral",10.1109/SANER.2018.8330219,"25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings","Syntax errors are made by novice and experienced programmers alike; however, novice programmers lack the years of experience that help them quickly resolve these frustrating errors. Standard LR parsers are of little help, typically resolving syntax errors and their precise location poorly. We propose a methodology that locates where syntax errors occur, and suggests possible changes to the token stream that can fix the error identified. This methodology finds syntax errors by using language models trained on correct source code to find tokens that seem out of place. Fixes are synthesized by consulting the language models to determine what tokens are more likely at the estimated error location. We compare n-gram and LSTM (long short-term memory) language models for this task, each trained on a large corpus of Java code collected from GitHub. Unlike prior work, our methodology does not rely that the problem source code comes from the same domain as the training data. We evaluated against a repository of real student mistakes. Our tools are able to find a syntactically-valid fix within its top-2 suggestions, often producing the exact fix that the student used to resolve the error. The results show that this tool and methodology can locate and suggest corrections for syntax errors. Our methodology is of practical use to all programmers, but will be especially useful to novices frustrated with incomprehensible syntax errors.",,311-322,,,Santos2018,Institute of Electrical and Electronics Engineers Inc.,2018-March,,article,,2018,,Syntax and sensibility: Using language models to detect and correct syntax errors,4,9781538649695,Backward,Include,Supervised,Both,N-gram (10-gram) and LSTM,Normalize tokens,Syntactic Correctness,Mean Reciprocal Rank; Accuracy,"Github for training, Blackbox for testing"
"Umair Z. Ahmed, Pawan Kumar, Amey Karkare, Purushottam Kar, Sumit Gulwani",10.1145/3183377.3183383,Proceedings - International Conference on Software Engineering,"Compile-time errors pose a major learning hurdle for students of introductory programming courses. Compiler error messages, while accurate, are targeted at seasoned programmers, and seem cryptic to beginners. In thiswork,we address this problem of pedagogicallyinspired program repair and report TRACER (Targeted RepAir of Compilation ERrors), a system for performing repairs on compilation errors, aimed at introductory programmers. TRACER invokes a novel combination of tools from programming language theory and deep learning and offers repairs that not only enable successful compilation, but repairs that are very close to those actually performed by students on similar errors. The ability to offer such targeted corrections, rather than just code that compiles, makes TRACER more relevant in offering real-time feedback to students in lab or tutorial sessions, as compared to existing works that merely offer a certain compilation success rate. In an evaluation on 4500 erroneous C programs written by students of a freshman year programming course, TRACER recommends a repair exactly matching the one expected by the student for 68% of the cases, and in 79.27% of the cases, produces a compilable repair. On a further set of 6971 programs that require errors to be fixed on multiple lines, TRACER enjoyed a success rate of 44% compared to the 27% success rate offered by the state-of-the-art technique DeepFix.",,78-87,2705257,https://dl.acm.org/doi/10.1145/3183377.3183383,Ahmed2018,IEEE Computer Society,18,,article,"Automatic Repair,Compilation Errors,Intelligent Tutoring Systems,Recommendation Systems,• Social and professional topics → CS1",2018,,"Compilation error repair: For the student programs, from the student programs",5,9781450356602,Backward,Include,Supervised,Neural,Encoder/Decoder with attention,Normalize tokens; String literals removed,Syntactic Correctness,Perc@k; SPerc@k; Compared to different datasets; Compared to baselines,Internal
"Artyom Lobanov, Timofey Bryksin, Alexey Shpilman",10.1007/978-3-030-23207-8_33/FIGURES/2,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Online programming courses are becoming more and more popular, but they still have significant drawbacks when compared to the traditional education system, e.g., the lack of feedback. In this study, we apply machine learning methods to improve the feedback of automated verification systems for programming assignments. We propose an approach that provides an insight on how to fix the code for a given incorrect submission. To achieve this, we detect frequent error types by clustering previously submitted incorrect solutions, label these clusters and use this labeled dataset to identify the type of an error in a new submission. We examine and compare several approaches to the detection of frequent error types and to the assignment of clusters to new submissions. The proposed method is evaluated on a dataset provided by a popular online learning platform.",,174-178,16113349,https://link.springer.com/chapter/10.1007/978-3-030-23207-8_33,Lobanov2019,Springer Verlag,11626 LNAI,,article,"Automatic evaluation,Classification,Clustering,MOOC,Programming",2019,,Automatic classification of error types in solutions to programming assignments at online learning platform,,9783030232061,SLR,Include,Unsupervised,Traditional,Heirarchal Agg Clustering,"Convert to Source to Graph (AST, CFG or DDG)",Correctness,Precision-Recall Curve; Compared to Validation Set Only,Stepik
"Stephanie Rogers, Dan Garcia, John F Canny, Steven Tang, Daniel Kang",,,,,,,http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-77.html,Rogers2014,,,,thesis,,2014,,ACES: Automatic Evaluation of Coding Style,,,Backward,Exclude - Not Peer Reviewed (master’s thesis?),,,,,,,
"Gilbert Cruz, Jacob Jones, Meagan Morrow, Andres Gonzalez, Bruce Gooch",10.1007/978-3-319-58515-4_2/FIGURES/3,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"We inhabit a century where every job will be technical. In the 21st century, learning to program a computer is empowerment. Programming instruction teaches procedural and functional thinking, project management and time management, skills that are essential components of an empowered individual. Programming is the power to create, the power to change and the power to influence. Today’s students regardless of their field of study or need this fundamental knowledge. Rapidly giving students meaningful feedback is a fundamental component of an effective educational experience. A common problem in modern education is scalability, as class size increases an instructor’s ability to provide meaningful feedback decreases. We report on an online Artificial Intelligence (AI) system capable of providing insightful narrative based coaching to beginning programmers. We document system tests to ensure that: it generates a unique response to every input, makes responses in real time, and is deployable online.",,12-21,16113349,https://link.springer.com/chapter/10.1007/978-3-319-58515-4_2,rayyan-354359413,Springer Verlag,10296 LNCS,,article,"Coaching,Education,Feedback,Programming",2017,,An AI system for coaching novice programmers,,9783319585147,SLR,Exclude - Lacking detail on ML implementation,,,,,,,
"Yu Dong, Jingyang Hou, Xuesong Lu",10.1007/978-3-030-59419-0_57/FIGURES/3,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Online judge (OJ) systems are becoming increasingly popular in various applications such as programming training, competitive programming contests and even employee recruitment, mainly due to their ability of automatic evaluation of code submissions. In higher education, OJ systems have been extensively used in programming courses because the automatic evaluation feature can drastically reduce the grading workload of instructors and teaching assistants and thereby makes the class size scalable. However, in our teaching we feel that existing OJ systems should improve their ability on giving feedback to students and teachers, especially on code errors and knowledge states. The lack of such automatic feedback increases teachers’ involvement and thus prevents college programming training from being more scalable. To tackle this challenge, we leverage historical student data obtained from our OJ system and implement two automated functions, namely, code error prediction and student knowledge tracing, using machine learning models. We demonstrate how students and teachers may benefit from the adoption of these two functions during programming training.",,785-789,16113349,https://link.springer.com/chapter/10.1007/978-3-030-59419-0_57,Dong2020,Springer Science and Business Media Deutschland GmbH,12114 LNCS,,article,"Error prediction,Intelligent online judge,Knowledge tracing",2020,,An Intelligent Online Judge System for Programming Training,,9783030594183,SLR,Exclude - Lacking detail on ML implementation,,,,,,,
"Eerik Muuli, Kaspar Papli, Eno Tõnisson, Marina Lepp, Tauno Palts, Reelika Suviste, Merilin Säde, Piret Luik",10.1007/978-3-319-66610-5_12/TABLES/1,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Automatic assessment of programming tasks in MOOCs (Massive Open Online Courses) is essential due to the large number of submissions. However, this often limits the scope of the assignments since task requirements must be strict for the solutions to be automatically gradable, reducing the opportunity for solutions to be creative. In order to alleviate this problem, we introduce a system capable of assessing the graphical output of a solution program using image recognition. This idea is applied to introductory computer graphics programming tasks whose solutions are programs that produce images of a given object on the screen. The image produced by the solution program is analysed using image recognition, resulting in a probability of a given object appearing in the image. The solution is accepted or rejected based on this score. The system was tested in a MOOC on 2,272 solution submissions. The results contained 4.6% cases of false negative and 0.5% cases of false positive grades. The method introduced in this paper saved approximately one minute per submission of the instructors’ time compared to manual grading. A participant survey revealed that the system was perceived to be functioning well or very well by 82.1% of the respondents, with an average rating of 4.4 out of 5.",,153-163,16113349,https://link.springer.com/chapter/10.1007/978-3-319-66610-5_12,rayyan-354359310,Springer Verlag,10474 LNCS,,article,"Automatic assessment,Automatic grading,Computer graphics,Image recognition,MOOC,Programming",2017,,Automatic assessment of programming assignments using image recognition,,9783319666099,SLR,Exclude - Lacking detail on ML implementation,,,,,,,
"Ted Carmichael, Mary Jean Blink, John Stamper, Elizabeth Gieske",,,"Linkage Objects for Generalized Instruction in Coding (LOGIC) is an intelligent system for online tutoring which detects errors among programming exercises to improve understanding of student progress. This system represents an implementation of the Hint Factory method for automated hint generation. In this approach , variables and their dependencies are abstracted from correct coding solutions to determine all the possible paths towards a solution, regardless of the programming language or variable names. Incomplete programs can be compared to these unique paths after code normalization, and the next best line can be supplied in the form of a hint. Errors are recorded based on discrepancy between best-match and the student's code. The final report categorizing all errors is compiled to benefit the teacher's effectiveness , highlighting common errors made by students.",,,,www.aaai.org,rayyan-354359409,,,,article,Intelligent Learning Technologies,2018,,Linkage Objects for Generalized Instruction in Coding (LOGIC),,,SLR,Exclude - Lacking detail on ML implementation,,,,,,,
"Florin Bulgarov, Rodney Nielsen",10.1609/AAAI.V32I1.12009,Proceedings of the AAAI Conference on Artificial Intelligence,"The next generation of educational applications need to significantly improve the way feedback is offered to both teachers and students. Simply determining coarse-grained entailment relations between the teacher's reference answer as a whole and a student response will not be sufficient. A finer-grained analysis is needed to determine which aspects of the reference answer have been understood and which have not. To this end, we propose an approach that splits the reference answer into its constituent propositions and two methods for detecting entailment relations between each reference answer proposition and a student response. Both methods, one using hand-crafted features and an SVM and the other using word embeddings and deep neural networks, achieve significant improvements over a state-of-the-art system and two alternative approaches.",,,2374-3468,https://ojs.aaai.org/index.php/AAAI/article/view/12009,Bulgarov2018,,32,,article,"deep neural networks,entailment,machine learning,word embeddings",2018,1,Proposition Entailment in Educational Applications using Deep Neural Networks,4,,Backward,Exclude - Not programming,,,,,,,
"Yewen Pu, Karthik Narasimhan, Armando Solar-Lezama, Regina Barzilay",10.1145/2984043.2989222,"SPLASH Companion 2016 - Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity","We present a novel technique for automatic program correction in MOOCs, capable of fixing both syntactic and semantic errors without manual, problem specific correction strategies. Given an incorrect student program, it generates candidate programs from a distribution of likely corrections, and checks each candidate for correctness against a test suite. The key observation is that in MOOCs many programs share similar code fragments, and the seq2seq neural network model, used in the natural-language processing task of machine translation, can be modified and trained to recover these fragments. Experiment shows our scheme can correct 29% of all incorrect submissions and out-performs state of the art approach which requires manual, problem specific correction strategies.",,39-40,,https://dl.acm.org/doi/10.1145/2984043.2989222,Pu2016,"Association for Computing Machinery, Inc",,,article,"Code repair and completion,Language model,MOOCs",2016,,Sk_P: A neural program corrector for MOOCs,10,9781450344371,Backward,Exclude - Lacking detail on ML implementation,,,,,,,